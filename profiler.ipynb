{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variables before using the transformers library\n",
    "os.environ[\"HF_HOME\"] = \"/serenity/scratch/hkolisetty6/.cache/huggingface\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"./transformers/src\")\n",
    "sys.path.insert(0, \"./peft/src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/serenity/scratch/dgarg/anaconda3/envs/amoaballm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from os.path import isdir, join\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import argparse\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    set_seed,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "from peft import (\n",
    "    PeftModel\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "\n",
    "from main import (\n",
    "    ModelArguments, \n",
    "    DataArguments, \n",
    "    TrainingArguments, \n",
    "    GenerationArguments, \n",
    "    smart_tokenizer_and_embedding_resize,\n",
    "    DEFAULT_PAD_TOKEN,\n",
    "    make_data_module,\n",
    "    load_dataset,\n",
    "    TokenTimingStoppingCriteria,\n",
    "    eval_all,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compute_dtype(args):\n",
    "    compute_dtype = (torch.float16 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32))\n",
    "    if torch.cuda.is_bf16_supported() and compute_dtype == torch.float16:\n",
    "        print('GPU supports bfloat16, so switching the compute_dtype to torch.bfloat16')\n",
    "        compute_dtype = torch.bfloat16\n",
    "    return compute_dtype\n",
    "\n",
    "def parse_args(args_list=None):\n",
    "    '''\n",
    "    Parse the command line arguments for the Hugging Face model, data, training, and generation arguments\n",
    "    '''\n",
    "    # Create an argument parser for the Hugging Face model, data, training, and generation arguments\n",
    "    hfparser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments, GenerationArguments))\n",
    "\n",
    "    # Parse the command line arguments into the respective dataclass instances\n",
    "    if args_list is None:\n",
    "        model_args, data_args, training_args, generation_args, _ = \\\n",
    "            hfparser.parse_args_into_dataclasses(return_remaining_strings=True)\n",
    "    else:\n",
    "        model_args, data_args, training_args, generation_args, _ = \\\n",
    "            hfparser.parse_args_into_dataclasses(args_list, return_remaining_strings=True)\n",
    "    training_args.generation_config = transformers.GenerationConfig(\n",
    "        **vars(generation_args)\n",
    "    )\n",
    "\n",
    "    # Combine the parsed arguments into a single argparse.Namespace object\n",
    "    args = argparse.Namespace(\n",
    "        **vars(model_args), **vars(data_args), **vars(training_args)\n",
    "    )\n",
    "\n",
    "    # Set additional parameters for later use\n",
    "    args.compute_dtype = get_compute_dtype(args)\n",
    "\n",
    "    return args, training_args\n",
    "\n",
    "def get_last_checkpoint(output_dir):\n",
    "    '''\n",
    "    Given the output directory, return the last checkpoint directory\n",
    "    '''\n",
    "    assert output_dir is not None, 'Output directory must be specified'\n",
    "    assert isdir(output_dir), 'Output directory does not exist'\n",
    "\n",
    "    max_steps = 0\n",
    "    for filename in os.listdir(output_dir):\n",
    "        if isdir(join(output_dir, filename)) and filename.startswith('checkpoint-'):\n",
    "            step = int(filename.split('-')[-1])\n",
    "            if step > max_steps:\n",
    "                max_steps = step\n",
    "    assert max_steps > 0, 'No checkpoints found in output directory'\n",
    "    last_checkpoint_dir = join(output_dir, f'checkpoint-{max_steps}')\n",
    "    return last_checkpoint_dir\n",
    "\n",
    "def get_tokenizer(args, model):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path=args.model_name_or_path,\n",
    "        token=args.use_auth_token,\n",
    "        padding_side='right',\n",
    "        tokenizer_type='llama' if 'llama' in args.model_name_or_path else None, # Needed for HF name change\n",
    "    )\n",
    "    if tokenizer._pad_token is None:\n",
    "        smart_tokenizer_and_embedding_resize(\n",
    "            special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "            tokenizer=tokenizer,\n",
    "            model=model,\n",
    "        )\n",
    "    if 'llama' in args.model_name_or_path or isinstance(tokenizer, LlamaTokenizer):\n",
    "        # LLaMA tokenizer may not have correct special tokens set.\n",
    "        # Check and add them if missing to prevent them from being parsed into different tokens.\n",
    "        # Note that these are present in the vocabulary.\n",
    "        # Note also that `model.config.pad_token_id` is 0 which corresponds to `<unk>` token.\n",
    "        print('Adding correct special tokens to the llama tokenizer')\n",
    "        tokenizer.add_special_tokens({\n",
    "                \"eos_token\": tokenizer.convert_ids_to_tokens(model.config.eos_token_id),\n",
    "                \"bos_token\": tokenizer.convert_ids_to_tokens(model.config.bos_token_id),\n",
    "        })\n",
    "    return tokenizer\n",
    "\n",
    "# Assumes the pruning width method is flap\n",
    "def set_width_mask_and_bias(model, args):\n",
    "    '''\n",
    "    Set the width_mask and bias for the model\n",
    "\n",
    "    hkolisetty6:\n",
    "    My understanding is that each module will hold the width_mask and bias for the corresponding layer.\n",
    "    The width_mask and bias are dicts with keys as width_ratio and values as the mask/bias tensor.\n",
    "    During inference, the width_mask and bias are applied to the output of the module.\n",
    "    In essense, full matrix multiplication is done and then the width_mask and bias are applied \n",
    "    to the output before passing it to the next layer.\n",
    "    '''\n",
    "    shrink_file = np.load(args.shrinking_file, allow_pickle=True).item()\n",
    "    assert 'width_mask' in shrink_file, 'Width mask not found in shrinking file'\n",
    "    assert 'bias' in shrink_file, 'Bias not found in shrinking file'\n",
    "\n",
    "    width_mask = shrink_file['width_mask']\n",
    "    bias = shrink_file['bias']\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if name in width_mask:\n",
    "            mask_dtype = args.compute_dtype # TODO hkolisetty6: in original code, this is set to torch.float32 when args.fp16 is True\n",
    "            if 'mlp.down_proj' in name or 'self_attn.o_proj' in name:\n",
    "                assert width_mask[name] is None\n",
    "                for key in bias[name].keys():\n",
    "                    bias[name][key] = torch.from_numpy(bias[name][key]).to(mask_dtype)\n",
    "                module.set_width_mask(width_mask=None, output_bias=bias[name])\n",
    "            else:\n",
    "                assert bias[name] is None\n",
    "                for key in width_mask[name].keys():\n",
    "                    width_mask[name][key] = torch.from_numpy(width_mask[name][key]).to(mask_dtype)\n",
    "                module.set_width_mask(width_mask=width_mask[name], output_bias=None)\n",
    "\n",
    "\n",
    "# Assumes model is loaded on a single GPU\n",
    "def load_model(args, checkpoint_dir):\n",
    "    '''\n",
    "    Given the command line arguments and the checkpoint directory, return the model\n",
    "    '''\n",
    "    shrink_config = {\n",
    "        'enable_shrinking': args.enable_shrinking,\n",
    "        'shrinkable_width': args.shrinkable_width,\n",
    "        'shrinking_method': args.shrinking_method,\n",
    "        'shrinking_file': args.shrinking_file,\n",
    "        'mask_dtype': str(args.compute_dtype),\n",
    "    }\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=args.bits == 4,\n",
    "        load_in_8bit=args.bits == 8,\n",
    "        llm_int8_threshold=6.0,\n",
    "        llm_int8_has_fp16_weight=False,\n",
    "        bnb_4bit_compute_dtype=args.compute_dtype,\n",
    "        bnb_4bit_use_double_quant=args.double_quant,\n",
    "        bnb_4bit_quant_type=args.quant_type,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=args.model_name_or_path,\n",
    "        device_map='auto',\n",
    "        torch_dtype=args.compute_dtype, # TODO hkolisetty6: in original code, this is set to torch.float32\n",
    "        token=args.use_auth_token,\n",
    "        shrink_config=shrink_config,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "    model.config.torch_dtype = args.compute_dtype\n",
    "\n",
    "    # !IMPORTANT \n",
    "    # Load the tokenizer before loading the adapters to ensure that the special tokens, if not available, are added to embeddings\n",
    "    # This is important for the adapters to be loaded correctly\n",
    "    tokenizer = get_tokenizer(args, model)\n",
    "\n",
    "    # Load adapters from checkpoint\n",
    "    print('Loading adapters from checkpoint')\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model=model,\n",
    "        model_id=join(checkpoint_dir, 'adapter_model'),\n",
    "    )\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LoraLayer) or 'lm_head' in name or 'embed_tokens' in name:\n",
    "            if args.compute_dtype == torch.bfloat16:\n",
    "                module = module.to(torch.float32)\n",
    "        if 'norm' in name:\n",
    "            module = module.to(torch.float32)\n",
    "    \n",
    "    model.config.use_cache = False # Check LlamaConfig for this attribute\n",
    "\n",
    "    # Set the width mask and bias for the model\n",
    "    set_width_mask_and_bias(model, args)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_mmlu_dataset_for_evaluation(args, tokenizer):\n",
    "    if args.do_mmlu_eval:\n",
    "        if args.mmlu_dataset == 'mmlu-zs':\n",
    "            mmlu_dataset = load_dataset(\"json\", data_files={\n",
    "                'eval': 'data/mmlu/zero_shot_mmlu_val.json',\n",
    "                'test': 'data/mmlu/zero_shot_mmlu_test.json',\n",
    "            })\n",
    "            mmlu_dataset = mmlu_dataset.remove_columns('subject')\n",
    "        # MMLU Five-shot (Eval/Test only)\n",
    "        elif args.mmlu_dataset == 'mmlu' or args.mmlu_dataset == 'mmlu-fs':\n",
    "            mmlu_dataset = load_dataset(\"json\", data_files={\n",
    "                'eval': 'data/mmlu/five_shot_mmlu_val.json',\n",
    "                'test': 'data/mmlu/five_shot_mmlu_test.json',\n",
    "            })\n",
    "            # mmlu_dataset = mmlu_dataset.remove_columns('subject')\n",
    "        mmlu_dataset = mmlu_dataset[args.mmlu_split]\n",
    "        if args.max_mmlu_samples is not None:\n",
    "            mmlu_dataset = mmlu_dataset.select(range(args.max_mmlu_samples))\n",
    "        abcd_idx = [\n",
    "            tokenizer(\"A\", add_special_tokens=False).input_ids[0],\n",
    "            tokenizer(\"B\", add_special_tokens=False).input_ids[0],\n",
    "            tokenizer(\"C\", add_special_tokens=False).input_ids[0],\n",
    "            tokenizer(\"D\", add_special_tokens=False).input_ids[0],\n",
    "        ]\n",
    "        accuracy = evaluate.load(\"accuracy\")\n",
    "        return mmlu_dataset, abcd_idx, accuracy\n",
    "    return None, None, None\n",
    "\n",
    "def eval_model(trainer, args, logger, all_metrics):\n",
    "    if args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "        metrics = trainer.evaluate(metric_key_prefix=\"eval\")\n",
    "        trainer.log_metrics(\"eval\", metrics)\n",
    "        trainer.save_metrics(\"eval\", metrics)\n",
    "        all_metrics.update(metrics)\n",
    "\n",
    "# Assumes shrinking_method is calib_dp and shrinking is enabled\n",
    "def setup_model_for_inference(model, args):\n",
    "    '''\n",
    "    Setup model for inference by activating the layers and setting the width ratio\n",
    "    '''\n",
    "    strategy = np.load(args.shrinking_file, allow_pickle=True).item()[\"strategy\"]\n",
    "    if 0 not in list(strategy.keys()):\n",
    "        strategy[0] = np.ones(model.config.num_hidden_layers)\n",
    "\n",
    "    active_layers_attn = active_layers_mlp = strategy[\n",
    "        model.config.num_hidden_layers - args.eval_num_layer\n",
    "    ]\n",
    "\n",
    "    if args.shrinkable_width:\n",
    "        for module in model.modules():\n",
    "            if hasattr(module, 'set_width_ratio'):\n",
    "                module.set_width_ratio(width_ratio=args.eval_num_width)\n",
    "        model.set_active_layers(\n",
    "            active_layers_attn, active_layers_mlp, width=args.eval_num_width\n",
    "        )\n",
    "    else:\n",
    "        model.set_active_layers(active_layers_attn, active_layers_mlp)\n",
    "\n",
    "\n",
    "def profile_latencies(model, tokenizer, args, logger, trainer, data_module):\n",
    "    '''\n",
    "    Given depth and width ratios (in args), profile the model for TTFT and TBT latencies\n",
    "    '''\n",
    "    setup_model_for_inference(model, args)\n",
    "    logger.info(\"Profiling model for TTFT and TBT latencies\")\n",
    "    timing_stopping_criteria = TokenTimingStoppingCriteria()\n",
    "    prediction_output = trainer.predict(\n",
    "        test_dataset=data_module[\"predict_dataset\"],\n",
    "        metric_key_prefix=\"predict\",\n",
    "        stopping_criteria=[timing_stopping_criteria],\n",
    "    )\n",
    "\n",
    "    print(timing_stopping_criteria.ttft)\n",
    "    print(timing_stopping_criteria.tbt)\n",
    "\n",
    "    prediction_metrics = prediction_output.metrics\n",
    "    predictions = prediction_output.predictions\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "\n",
    "    predictions = tokenizer.batch_decode(\n",
    "        predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    with open(os.path.join(args.output_dir, 'predictions_test.jsonl'), 'w') as fout:\n",
    "        for i, example in enumerate(data_module['predict_dataset']):\n",
    "            example['prediction_with_input'] = predictions[i].strip()\n",
    "            example['prediction'] = predictions[i].replace(example['input'], '').strip()\n",
    "            fout.write(json.dumps(example) + '\\n')\n",
    "    print(prediction_metrics)\n",
    "    trainer.log_metrics(\"predict\", prediction_metrics)\n",
    "    trainer.save_metrics(\"predict\", prediction_metrics)\n",
    "\n",
    "    return timing_stopping_criteria.ttft, timing_stopping_criteria.tbt\n",
    "\n",
    "def profile_accuracies(model, tokenizer, args, trainer, logger, mmlu_dataset, abcd_idx, accuracy, all_metrics):\n",
    "    logger.info(\"Profiling model for accuracies\")\n",
    "    setup_model_for_inference(model, args)\n",
    "    num_layer = args.eval_num_layer\n",
    "    width = args.eval_num_width\n",
    "    all_metrics = eval_all(args, model, trainer, tokenizer, mmlu_dataset, abcd_idx=abcd_idx, accuracy=accuracy, all_metrics=all_metrics, suffix=f'_l{num_layer}w{width}')\n",
    "\n",
    "    with open(os.path.join(args.output_dir, \"metrics.json\"), \"w\") as fout:\n",
    "        fout.write(json.dumps(all_metrics))\n",
    "    return all_metrics\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/serenity/scratch/dgarg/anaconda3/envs/amoaballm/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding correct special tokens to the llama tokenizer\n",
      "Loading adapters from checkpoint\n"
     ]
    }
   ],
   "source": [
    "args = [\n",
    "    \"--model_name_or_path\", \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"--output_dir\", \"amoeba_llama2\",\n",
    "    \"--do_predict\", \"True\",\n",
    "    \"--do_eval\", \"False\",\n",
    "    \"--do_train\", \"False\",\n",
    "    \"--do_mmlu_eval\", \"True\",\n",
    "    \"--enable_shrinking\",\n",
    "    \"--min_num_layer\", \"20\",\n",
    "    \"--shrinking_method\", \"calib_dp\",\n",
    "    \"--shrinking_file\", \"dp_selection_strategy.npy\",\n",
    "    \"--shrinkable_width\",\n",
    "    \"--width_choice\", \"[1,7/8,3/4,5/8,1/2]\",\n",
    "    \"--prune_width_method\", \"flap\",\n",
    "    \"--use_moe_lora\",\n",
    "    \"--moe_num_expert\", \"5\",\n",
    "    \"--moe_topk\", \"2\",\n",
    "    \"--eval_num_layer\", \"32\",\n",
    "    \"--eval_num_width\", \"1\",\n",
    "]\n",
    "\n",
    "args, training_args = parse_args(args_list=args)\n",
    "checkpoint_dir = get_last_checkpoint(args.output_dir)\n",
    "model, tokenizer = load_model(args, checkpoint_dir)\n",
    "set_width_mask_and_bias(model, args)\n",
    "\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [10:01<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l32w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.4483\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.0909\n",
      "  mmlu_eval_accuracy_anatomy                             =    0.5\n",
      "  mmlu_eval_accuracy_astronomy                           = 0.3125\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.4545\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.4483\n",
      "  mmlu_eval_accuracy_college_biology                     =   0.25\n",
      "  mmlu_eval_accuracy_college_chemistry                   =   0.25\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.1818\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.2727\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.4091\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.3636\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.3636\n",
      "  mmlu_eval_accuracy_conceptual_physics                  =    0.5\n",
      "  mmlu_eval_accuracy_econometrics                        =   0.25\n",
      "  mmlu_eval_accuracy_electrical_engineering              = 0.3125\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.3902\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.2143\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.2\n",
      "  mmlu_eval_accuracy_high_school_biology                 = 0.4688\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.4091\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.4444\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.7222\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.7273\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics =  0.619\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.4186\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.2069\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3462\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.2353\n",
      "  mmlu_eval_accuracy_high_school_psychology              =   0.65\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.3478\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.5909\n",
      "  mmlu_eval_accuracy_high_school_world_history           = 0.6538\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.7826\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.4167\n",
      "  mmlu_eval_accuracy_international_law                   = 0.7692\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.3636\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.6111\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.1818\n",
      "  mmlu_eval_accuracy_management                          = 0.6364\n",
      "  mmlu_eval_accuracy_marketing                           =    0.8\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.7273\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.6047\n",
      "  mmlu_eval_accuracy_moral_disputes                      =    0.5\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.24\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.6667\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.4412\n",
      "  mmlu_eval_accuracy_prehistory                          =    0.4\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.3226\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.3118\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.4516\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.4638\n",
      "  mmlu_eval_accuracy_public_relations                    = 0.4167\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.4815\n",
      "  mmlu_eval_accuracy_sociology                           = 0.6818\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.5455\n",
      "  mmlu_eval_accuracy_virology                            =    0.5\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.6316\n",
      "  mmlu_loss                                              = 1.8114\n"
     ]
    }
   ],
   "source": [
    "all_metrics = {}\n",
    "\n",
    "data_module = make_data_module(tokenizer, args)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    **{k: v for k, v in data_module.items() if k != \"predict_dataset\"},\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "mmlu_dataset, abcd_idx, accuracy = get_mmlu_dataset_for_evaluation(args, tokenizer)\n",
    "all_metrics = profile_accuracies(model, tokenizer, args, trainer, logger, mmlu_dataset, abcd_idx, accuracy, all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [05:06<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l16w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.2934\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.2727\n",
      "  mmlu_eval_accuracy_anatomy                             = 0.2857\n",
      "  mmlu_eval_accuracy_astronomy                           =   0.25\n",
      "  mmlu_eval_accuracy_business_ethics                     =    0.0\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.3793\n",
      "  mmlu_eval_accuracy_college_biology                     = 0.3125\n",
      "  mmlu_eval_accuracy_college_chemistry                   =  0.625\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.3636\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.4545\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.1818\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.3636\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.2727\n",
      "  mmlu_eval_accuracy_conceptual_physics                  = 0.3077\n",
      "  mmlu_eval_accuracy_econometrics                        =   0.25\n",
      "  mmlu_eval_accuracy_electrical_engineering              = 0.3125\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.1707\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.3571\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.3\n",
      "  mmlu_eval_accuracy_high_school_biology                 = 0.2188\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.1364\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.2222\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.3889\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.2727\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.1429\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.2791\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.2759\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.1538\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.2353\n",
      "  mmlu_eval_accuracy_high_school_psychology              = 0.3167\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.3913\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.2273\n",
      "  mmlu_eval_accuracy_high_school_world_history           = 0.4231\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.2609\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.4167\n",
      "  mmlu_eval_accuracy_international_law                   = 0.3846\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.1818\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.3333\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.3636\n",
      "  mmlu_eval_accuracy_management                          = 0.4545\n",
      "  mmlu_eval_accuracy_marketing                           =   0.24\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.3636\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.4651\n",
      "  mmlu_eval_accuracy_moral_disputes                      = 0.2895\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.23\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.3333\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.2059\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.3143\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.3548\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.2588\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.3226\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.2464\n",
      "  mmlu_eval_accuracy_public_relations                    =   0.25\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.2593\n",
      "  mmlu_eval_accuracy_sociology                           = 0.3636\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.0909\n",
      "  mmlu_eval_accuracy_virology                            = 0.2778\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.3158\n",
      "  mmlu_loss                                              = 2.3896\n"
     ]
    }
   ],
   "source": [
    "args.eval_num_width = 1.0\n",
    "args.eval_num_layer = 16\n",
    "all_metrics = profile_accuracies(model, tokenizer, args, trainer, logger, mmlu_dataset, abcd_idx, accuracy, all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [07:33<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l24w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.4307\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.0909\n",
      "  mmlu_eval_accuracy_anatomy                             = 0.4286\n",
      "  mmlu_eval_accuracy_astronomy                           =  0.375\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.6364\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.3793\n",
      "  mmlu_eval_accuracy_college_biology                     =  0.375\n",
      "  mmlu_eval_accuracy_college_chemistry                   =  0.375\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.3636\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.2727\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.4091\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.6364\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.3636\n",
      "  mmlu_eval_accuracy_conceptual_physics                  = 0.3846\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.0833\n",
      "  mmlu_eval_accuracy_electrical_engineering              =  0.375\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.2683\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.2143\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.1\n",
      "  mmlu_eval_accuracy_high_school_biology                 =  0.375\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.3636\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.4444\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.5556\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.6818\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.5238\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.4651\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.2759\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3846\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.2353\n",
      "  mmlu_eval_accuracy_high_school_psychology              = 0.5333\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.3913\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.6364\n",
      "  mmlu_eval_accuracy_high_school_world_history           = 0.4615\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.6957\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.3333\n",
      "  mmlu_eval_accuracy_international_law                   = 0.6923\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.2727\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.5556\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.2727\n",
      "  mmlu_eval_accuracy_management                          = 0.6364\n",
      "  mmlu_eval_accuracy_marketing                           =   0.76\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.7273\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.6279\n",
      "  mmlu_eval_accuracy_moral_disputes                      = 0.4474\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.25\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.5758\n",
      "  mmlu_eval_accuracy_philosophy                          =    0.5\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.2857\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.3548\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.2941\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.3871\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.4058\n",
      "  mmlu_eval_accuracy_public_relations                    = 0.3333\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.4815\n",
      "  mmlu_eval_accuracy_sociology                           = 0.5455\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.6364\n",
      "  mmlu_eval_accuracy_virology                            = 0.3333\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.6842\n",
      "  mmlu_loss                                              = 1.4714\n"
     ]
    }
   ],
   "source": [
    "args.eval_num_width = 1.0\n",
    "args.eval_num_layer = 24\n",
    "all_metrics = profile_accuracies(model, tokenizer, args, trainer, logger, mmlu_dataset, abcd_idx, accuracy, all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling model for accuracies with 18 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [05:42<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l18w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.3881\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.0909\n",
      "  mmlu_eval_accuracy_anatomy                             = 0.5714\n",
      "  mmlu_eval_accuracy_astronomy                           = 0.4375\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.4545\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.3793\n",
      "  mmlu_eval_accuracy_college_biology                     =  0.375\n",
      "  mmlu_eval_accuracy_college_chemistry                   =  0.375\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.3636\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.3636\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.2273\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.4545\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.5455\n",
      "  mmlu_eval_accuracy_conceptual_physics                  = 0.3077\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.1667\n",
      "  mmlu_eval_accuracy_electrical_engineering              =   0.25\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.3171\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.4286\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.1\n",
      "  mmlu_eval_accuracy_high_school_biology                 = 0.4062\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.1818\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.3333\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.3333\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.7273\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.3333\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.3488\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.2069\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.4231\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.2941\n",
      "  mmlu_eval_accuracy_high_school_psychology              =   0.45\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.2609\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.4545\n",
      "  mmlu_eval_accuracy_high_school_world_history           = 0.3462\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.4783\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.1667\n",
      "  mmlu_eval_accuracy_international_law                   = 0.8462\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.3636\n",
      "  mmlu_eval_accuracy_logical_fallacies                   =    0.5\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.1818\n",
      "  mmlu_eval_accuracy_management                          = 0.3636\n",
      "  mmlu_eval_accuracy_marketing                           =   0.68\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.6364\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.5233\n",
      "  mmlu_eval_accuracy_moral_disputes                      = 0.3421\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.31\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.4848\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.3235\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.5143\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.3226\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.2471\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.3871\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.3478\n",
      "  mmlu_eval_accuracy_public_relations                    = 0.5833\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.4815\n",
      "  mmlu_eval_accuracy_sociology                           = 0.4091\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.4545\n",
      "  mmlu_eval_accuracy_virology                            = 0.4444\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.4211\n",
      "  mmlu_loss                                              = 1.8062\n",
      "Profiling model for accuracies with 20 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [06:20<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l20w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.4082\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.0909\n",
      "  mmlu_eval_accuracy_anatomy                             = 0.5714\n",
      "  mmlu_eval_accuracy_astronomy                           = 0.3125\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.3636\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.4828\n",
      "  mmlu_eval_accuracy_college_biology                     = 0.5625\n",
      "  mmlu_eval_accuracy_college_chemistry                   =  0.375\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.2727\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.2727\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.4091\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.5455\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.6364\n",
      "  mmlu_eval_accuracy_conceptual_physics                  = 0.4615\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.0833\n",
      "  mmlu_eval_accuracy_electrical_engineering              = 0.3125\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.2927\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.2143\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.3\n",
      "  mmlu_eval_accuracy_high_school_biology                 = 0.4688\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.1818\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.3333\n",
      "  mmlu_eval_accuracy_high_school_european_history        =    0.5\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.6364\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics =  0.381\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.3953\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.2759\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3846\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.2353\n",
      "  mmlu_eval_accuracy_high_school_psychology              = 0.5333\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.3043\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.5455\n",
      "  mmlu_eval_accuracy_high_school_world_history           = 0.3462\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.4783\n",
      "  mmlu_eval_accuracy_human_sexuality                     =   0.25\n",
      "  mmlu_eval_accuracy_international_law                   = 0.7692\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.2727\n",
      "  mmlu_eval_accuracy_logical_fallacies                   =    0.5\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.1818\n",
      "  mmlu_eval_accuracy_management                          = 0.4545\n",
      "  mmlu_eval_accuracy_marketing                           =   0.64\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.5455\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.5814\n",
      "  mmlu_eval_accuracy_moral_disputes                      =    0.5\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.34\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.4545\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.4706\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.3429\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.3548\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.2765\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.3871\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.3478\n",
      "  mmlu_eval_accuracy_public_relations                    =    0.5\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.4444\n",
      "  mmlu_eval_accuracy_sociology                           = 0.6364\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.5455\n",
      "  mmlu_eval_accuracy_virology                            = 0.3333\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.5789\n",
      "  mmlu_loss                                              = 1.4432\n",
      "Profiling model for accuracies with 22 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [06:57<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l22w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.4396\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.0909\n",
      "  mmlu_eval_accuracy_anatomy                             = 0.5714\n",
      "  mmlu_eval_accuracy_astronomy                           =  0.375\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.4545\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.4483\n",
      "  mmlu_eval_accuracy_college_biology                     = 0.5625\n",
      "  mmlu_eval_accuracy_college_chemistry                   =  0.375\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.3636\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.1818\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.3636\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.6364\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.5455\n",
      "  mmlu_eval_accuracy_conceptual_physics                  = 0.4231\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.1667\n",
      "  mmlu_eval_accuracy_electrical_engineering              =  0.375\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.2927\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.2857\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.2\n",
      "  mmlu_eval_accuracy_high_school_biology                 = 0.4375\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.3182\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.4444\n",
      "  mmlu_eval_accuracy_high_school_european_history        =    0.5\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.7273\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.5238\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.4884\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.3103\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3077\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.2353\n",
      "  mmlu_eval_accuracy_high_school_psychology              =   0.55\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.5217\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.5909\n",
      "  mmlu_eval_accuracy_high_school_world_history           =    0.5\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.5652\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.3333\n",
      "  mmlu_eval_accuracy_international_law                   = 0.7692\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.2727\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.6111\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.1818\n",
      "  mmlu_eval_accuracy_management                          = 0.6364\n",
      "  mmlu_eval_accuracy_marketing                           =    0.8\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.7273\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.6163\n",
      "  mmlu_eval_accuracy_moral_disputes                      = 0.4737\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.24\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.5455\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.4118\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.3429\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.2581\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.2706\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.3226\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.4058\n",
      "  mmlu_eval_accuracy_public_relations                    = 0.3333\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.4815\n",
      "  mmlu_eval_accuracy_sociology                           = 0.7273\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.5455\n",
      "  mmlu_eval_accuracy_virology                            = 0.3889\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.6316\n",
      "  mmlu_loss                                              = 1.6915\n",
      "Profiling model for accuracies with 26 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [08:10<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l26w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     =  0.438\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.0909\n",
      "  mmlu_eval_accuracy_anatomy                             = 0.4286\n",
      "  mmlu_eval_accuracy_astronomy                           =  0.375\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.6364\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.3793\n",
      "  mmlu_eval_accuracy_college_biology                     = 0.3125\n",
      "  mmlu_eval_accuracy_college_chemistry                   =  0.625\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.4545\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.2727\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.4091\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.4545\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.3636\n",
      "  mmlu_eval_accuracy_conceptual_physics                  = 0.3077\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.0833\n",
      "  mmlu_eval_accuracy_electrical_engineering              =  0.375\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.3171\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.2143\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.1\n",
      "  mmlu_eval_accuracy_high_school_biology                 = 0.3438\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.3182\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.5556\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.7778\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.7273\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.5238\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.3721\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.2759\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3077\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.1176\n",
      "  mmlu_eval_accuracy_high_school_psychology              = 0.5833\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.4348\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.5909\n",
      "  mmlu_eval_accuracy_high_school_world_history           = 0.5769\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.6522\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.4167\n",
      "  mmlu_eval_accuracy_international_law                   = 0.6923\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.2727\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.5556\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.2727\n",
      "  mmlu_eval_accuracy_management                          = 0.6364\n",
      "  mmlu_eval_accuracy_marketing                           =   0.76\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.6364\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.6279\n",
      "  mmlu_eval_accuracy_moral_disputes                      = 0.3684\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.24\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.6061\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.4412\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.2857\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.3871\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.3176\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.4516\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.4058\n",
      "  mmlu_eval_accuracy_public_relations                    = 0.3333\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.4444\n",
      "  mmlu_eval_accuracy_sociology                           = 0.6364\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.6364\n",
      "  mmlu_eval_accuracy_virology                            = 0.4444\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.7368\n",
      "  mmlu_loss                                              = 1.6294\n",
      "Profiling model for accuracies with 28 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [08:47<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l28w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.4503\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.0909\n",
      "  mmlu_eval_accuracy_anatomy                             =    0.5\n",
      "  mmlu_eval_accuracy_astronomy                           = 0.3125\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.5455\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.3793\n",
      "  mmlu_eval_accuracy_college_biology                     = 0.3125\n",
      "  mmlu_eval_accuracy_college_chemistry                   =    0.5\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.4545\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.2727\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.3636\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.4545\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.3636\n",
      "  mmlu_eval_accuracy_conceptual_physics                  = 0.4615\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.1667\n",
      "  mmlu_eval_accuracy_electrical_engineering              =  0.375\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.3415\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.2857\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.3\n",
      "  mmlu_eval_accuracy_high_school_biology                 =  0.375\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.3636\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.3333\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.7778\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.6818\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.5238\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.3721\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.3103\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3077\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.1765\n",
      "  mmlu_eval_accuracy_high_school_psychology              = 0.6167\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.4348\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.6364\n",
      "  mmlu_eval_accuracy_high_school_world_history           = 0.6154\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.7391\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.4167\n",
      "  mmlu_eval_accuracy_international_law                   = 0.6923\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.3636\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.6111\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.1818\n",
      "  mmlu_eval_accuracy_management                          = 0.6364\n",
      "  mmlu_eval_accuracy_marketing                           =   0.76\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.7273\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.6279\n",
      "  mmlu_eval_accuracy_moral_disputes                      = 0.4737\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.24\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.6667\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.4412\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.3143\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.3548\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.3176\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.4194\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.4348\n",
      "  mmlu_eval_accuracy_public_relations                    = 0.3333\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.5185\n",
      "  mmlu_eval_accuracy_sociology                           = 0.6818\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.6364\n",
      "  mmlu_eval_accuracy_virology                            = 0.3889\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.6842\n",
      "  mmlu_loss                                              = 1.7815\n",
      "Profiling model for accuracies with 30 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [09:24<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l30w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.4497\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.1818\n",
      "  mmlu_eval_accuracy_anatomy                             =    0.5\n",
      "  mmlu_eval_accuracy_astronomy                           = 0.3125\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.5455\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.4138\n",
      "  mmlu_eval_accuracy_college_biology                     = 0.3125\n",
      "  mmlu_eval_accuracy_college_chemistry                   =   0.25\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.3636\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.2727\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.4091\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.3636\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.3636\n",
      "  mmlu_eval_accuracy_conceptual_physics                  =    0.5\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.1667\n",
      "  mmlu_eval_accuracy_electrical_engineering              = 0.3125\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.3902\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.2143\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.3\n",
      "  mmlu_eval_accuracy_high_school_biology                 = 0.4688\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.3636\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.3333\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.7778\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.6818\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.5238\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.4419\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.2759\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3846\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.2941\n",
      "  mmlu_eval_accuracy_high_school_psychology              =   0.65\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.3913\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.6364\n",
      "  mmlu_eval_accuracy_high_school_world_history           = 0.6538\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.6957\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.4167\n",
      "  mmlu_eval_accuracy_international_law                   = 0.7692\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.3636\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.6111\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.1818\n",
      "  mmlu_eval_accuracy_management                          = 0.6364\n",
      "  mmlu_eval_accuracy_marketing                           =   0.76\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.7273\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.6279\n",
      "  mmlu_eval_accuracy_moral_disputes                      =    0.5\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.24\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.6667\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.4412\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.3429\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.3226\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.2882\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.4194\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.4638\n",
      "  mmlu_eval_accuracy_public_relations                    = 0.3333\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.4815\n",
      "  mmlu_eval_accuracy_sociology                           = 0.6364\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.6364\n",
      "  mmlu_eval_accuracy_virology                            = 0.3889\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.6316\n",
      "  mmlu_loss                                              = 1.5835\n"
     ]
    }
   ],
   "source": [
    "for num_layers in [18, 20, 22, 26, 28, 30]:\n",
    "    args.eval_num_layer = num_layers\n",
    "    print(f\"Profiling model for accuracies with {num_layers} layers\")\n",
    "    all_metrics = profile_accuracies(model, tokenizer, args, trainer, logger, mmlu_dataset, abcd_idx, accuracy, all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n",
      "Profiling model for accuracies with 17 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [05:25<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l17w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.3351\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.3636\n",
      "  mmlu_eval_accuracy_anatomy                             = 0.7143\n",
      "  mmlu_eval_accuracy_astronomy                           = 0.3125\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.0909\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.4828\n",
      "  mmlu_eval_accuracy_college_biology                     =  0.375\n",
      "  mmlu_eval_accuracy_college_chemistry                   =  0.375\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.2727\n",
      "  mmlu_eval_accuracy_college_mathematics                 =    0.0\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.2727\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.7273\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.2727\n",
      "  mmlu_eval_accuracy_conceptual_physics                  = 0.3462\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.3333\n",
      "  mmlu_eval_accuracy_electrical_engineering              = 0.1875\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.2683\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.1429\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.0\n",
      "  mmlu_eval_accuracy_high_school_biology                 = 0.3438\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.1818\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.2222\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.3889\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.4545\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.3333\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.2326\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.2759\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3462\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.2353\n",
      "  mmlu_eval_accuracy_high_school_psychology              = 0.5167\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.2609\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.2273\n",
      "  mmlu_eval_accuracy_high_school_world_history           = 0.3077\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.3043\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.3333\n",
      "  mmlu_eval_accuracy_international_law                   = 0.6923\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.3636\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.3889\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.2727\n",
      "  mmlu_eval_accuracy_management                          = 0.4545\n",
      "  mmlu_eval_accuracy_marketing                           =   0.48\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.5455\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.4186\n",
      "  mmlu_eval_accuracy_moral_disputes                      = 0.4474\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.31\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.3939\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.2647\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.3429\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.1613\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.2706\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.3226\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.2609\n",
      "  mmlu_eval_accuracy_public_relations                    = 0.4167\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.1852\n",
      "  mmlu_eval_accuracy_sociology                           = 0.4545\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.4545\n",
      "  mmlu_eval_accuracy_virology                            = 0.2222\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.4737\n",
      "  mmlu_loss                                              = 2.4972\n",
      "Profiling model for accuracies with 19 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [06:02<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l19w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.4205\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.2727\n",
      "  mmlu_eval_accuracy_anatomy                             =    0.5\n",
      "  mmlu_eval_accuracy_astronomy                           =   0.25\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.4545\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.5172\n",
      "  mmlu_eval_accuracy_college_biology                     =  0.375\n",
      "  mmlu_eval_accuracy_college_chemistry                   =  0.375\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.4545\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.2727\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.3636\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.5455\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.5455\n",
      "  mmlu_eval_accuracy_conceptual_physics                  = 0.3077\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.1667\n",
      "  mmlu_eval_accuracy_electrical_engineering              = 0.3125\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.2927\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.2143\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.2\n",
      "  mmlu_eval_accuracy_high_school_biology                 = 0.3438\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.2727\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.3333\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.3333\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.7727\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.2857\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.3953\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.2069\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3846\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.1765\n",
      "  mmlu_eval_accuracy_high_school_psychology              = 0.5833\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.3043\n",
      "  mmlu_eval_accuracy_high_school_us_history              =    0.5\n",
      "  mmlu_eval_accuracy_high_school_world_history           = 0.4615\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.4348\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.3333\n",
      "  mmlu_eval_accuracy_international_law                   = 0.8462\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.4545\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.6111\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.3636\n",
      "  mmlu_eval_accuracy_management                          = 0.6364\n",
      "  mmlu_eval_accuracy_marketing                           =   0.72\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.6364\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.5349\n",
      "  mmlu_eval_accuracy_moral_disputes                      = 0.5526\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.32\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.5758\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.3824\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.3143\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.2903\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.2706\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.4194\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.4058\n",
      "  mmlu_eval_accuracy_public_relations                    = 0.6667\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.3333\n",
      "  mmlu_eval_accuracy_sociology                           = 0.7273\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.4545\n",
      "  mmlu_eval_accuracy_virology                            = 0.3333\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.5789\n",
      "  mmlu_loss                                              = 1.7562\n",
      "Profiling model for accuracies with 21 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [06:38<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l21w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.4311\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.0909\n",
      "  mmlu_eval_accuracy_anatomy                             = 0.5714\n",
      "  mmlu_eval_accuracy_astronomy                           =  0.375\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.3636\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.4828\n",
      "  mmlu_eval_accuracy_college_biology                     =    0.5\n",
      "  mmlu_eval_accuracy_college_chemistry                   =   0.25\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.3636\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.1818\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.3636\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.6364\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.4545\n",
      "  mmlu_eval_accuracy_conceptual_physics                  = 0.4231\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.1667\n",
      "  mmlu_eval_accuracy_electrical_engineering              =  0.375\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.2927\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.2143\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.2\n",
      "  mmlu_eval_accuracy_high_school_biology                 =    0.5\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.2727\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.4444\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.3889\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.7727\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.4762\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.4651\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.2414\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3462\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.3529\n",
      "  mmlu_eval_accuracy_high_school_psychology              = 0.5667\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.4783\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.5909\n",
      "  mmlu_eval_accuracy_high_school_world_history           = 0.4615\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.5217\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.4167\n",
      "  mmlu_eval_accuracy_international_law                   = 0.6923\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.2727\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.6111\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.1818\n",
      "  mmlu_eval_accuracy_management                          = 0.6364\n",
      "  mmlu_eval_accuracy_marketing                           =   0.76\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.7273\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.6163\n",
      "  mmlu_eval_accuracy_moral_disputes                      =    0.5\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.24\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.5455\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.4412\n",
      "  mmlu_eval_accuracy_prehistory                          =    0.4\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.2258\n",
      "  mmlu_eval_accuracy_professional_law                    =    0.3\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.3226\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.4058\n",
      "  mmlu_eval_accuracy_public_relations                    = 0.4167\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.5185\n",
      "  mmlu_eval_accuracy_sociology                           = 0.6818\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.4545\n",
      "  mmlu_eval_accuracy_virology                            = 0.3889\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.6316\n",
      "  mmlu_loss                                              = 1.5616\n",
      "Profiling model for accuracies with 23 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [07:15<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l23w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.4381\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.0909\n",
      "  mmlu_eval_accuracy_anatomy                             = 0.5714\n",
      "  mmlu_eval_accuracy_astronomy                           =  0.375\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.4545\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.3793\n",
      "  mmlu_eval_accuracy_college_biology                     =    0.5\n",
      "  mmlu_eval_accuracy_college_chemistry                   =  0.375\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.2727\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.2727\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.3636\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.5455\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.5455\n",
      "  mmlu_eval_accuracy_conceptual_physics                  =    0.5\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.1667\n",
      "  mmlu_eval_accuracy_electrical_engineering              =  0.375\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.2927\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.2857\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.2\n",
      "  mmlu_eval_accuracy_high_school_biology                 = 0.4375\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.2727\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.4444\n",
      "  mmlu_eval_accuracy_high_school_european_history        =    0.5\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.7727\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.4762\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.4651\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.2759\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3462\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.2941\n",
      "  mmlu_eval_accuracy_high_school_psychology              =   0.55\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.3478\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.5909\n",
      "  mmlu_eval_accuracy_high_school_world_history           =    0.5\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.6087\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.3333\n",
      "  mmlu_eval_accuracy_international_law                   = 0.6923\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.2727\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.6111\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.1818\n",
      "  mmlu_eval_accuracy_management                          = 0.5455\n",
      "  mmlu_eval_accuracy_marketing                           =   0.76\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.7273\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.6163\n",
      "  mmlu_eval_accuracy_moral_disputes                      =    0.5\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.25\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.6364\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.4706\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.3143\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.2581\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.2882\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.3548\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.3913\n",
      "  mmlu_eval_accuracy_public_relations                    = 0.4167\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.5185\n",
      "  mmlu_eval_accuracy_sociology                           = 0.7273\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.6364\n",
      "  mmlu_eval_accuracy_virology                            = 0.3333\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.6842\n",
      "  mmlu_loss                                              = 1.3676\n",
      "Profiling model for accuracies with 25 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [07:52<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l25w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.4377\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.0909\n",
      "  mmlu_eval_accuracy_anatomy                             = 0.4286\n",
      "  mmlu_eval_accuracy_astronomy                           = 0.3125\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.5455\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.4138\n",
      "  mmlu_eval_accuracy_college_biology                     = 0.4375\n",
      "  mmlu_eval_accuracy_college_chemistry                   =    0.5\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.3636\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.3636\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.3636\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.6364\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.3636\n",
      "  mmlu_eval_accuracy_conceptual_physics                  = 0.4231\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.1667\n",
      "  mmlu_eval_accuracy_electrical_engineering              =  0.375\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.3415\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.2143\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.1\n",
      "  mmlu_eval_accuracy_high_school_biology                 =  0.375\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.3636\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.4444\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.5556\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.7273\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.5238\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.4651\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.2414\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.4615\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.1765\n",
      "  mmlu_eval_accuracy_high_school_psychology              = 0.5333\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.3913\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.5909\n",
      "  mmlu_eval_accuracy_high_school_world_history           =    0.5\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.6957\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.3333\n",
      "  mmlu_eval_accuracy_international_law                   = 0.7692\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.2727\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.5556\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.2727\n",
      "  mmlu_eval_accuracy_management                          = 0.6364\n",
      "  mmlu_eval_accuracy_marketing                           =   0.76\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.7273\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.6279\n",
      "  mmlu_eval_accuracy_moral_disputes                      = 0.4474\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.24\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.6364\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.4706\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.2857\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.2581\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.2882\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.3871\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.4058\n",
      "  mmlu_eval_accuracy_public_relations                    =    0.5\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.4815\n",
      "  mmlu_eval_accuracy_sociology                           = 0.5455\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.5455\n",
      "  mmlu_eval_accuracy_virology                            = 0.3333\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.6842\n",
      "  mmlu_loss                                              = 1.4683\n",
      "Profiling model for accuracies with 27 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [08:28<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l27w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.4392\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.0909\n",
      "  mmlu_eval_accuracy_anatomy                             = 0.4286\n",
      "  mmlu_eval_accuracy_astronomy                           = 0.3125\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.4545\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.3793\n",
      "  mmlu_eval_accuracy_college_biology                     = 0.3125\n",
      "  mmlu_eval_accuracy_college_chemistry                   =  0.375\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.3636\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.2727\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.4091\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.3636\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.3636\n",
      "  mmlu_eval_accuracy_conceptual_physics                  = 0.4615\n",
      "  mmlu_eval_accuracy_econometrics                        =   0.25\n",
      "  mmlu_eval_accuracy_electrical_engineering              =  0.375\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.3902\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.2143\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.2\n",
      "  mmlu_eval_accuracy_high_school_biology                 = 0.4688\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.3636\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.5556\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.7222\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.7727\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.5238\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.3953\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.3448\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3846\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.1176\n",
      "  mmlu_eval_accuracy_high_school_psychology              =    0.6\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.3478\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.5909\n",
      "  mmlu_eval_accuracy_high_school_world_history           = 0.6538\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.6087\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.4167\n",
      "  mmlu_eval_accuracy_international_law                   = 0.6923\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.2727\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.5556\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.1818\n",
      "  mmlu_eval_accuracy_management                          = 0.6364\n",
      "  mmlu_eval_accuracy_marketing                           =   0.76\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.8182\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.6279\n",
      "  mmlu_eval_accuracy_moral_disputes                      = 0.4737\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.24\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.6667\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.4118\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.2857\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.3226\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.3118\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.3871\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.4203\n",
      "  mmlu_eval_accuracy_public_relations                    =   0.25\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.4815\n",
      "  mmlu_eval_accuracy_sociology                           = 0.6818\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.5455\n",
      "  mmlu_eval_accuracy_virology                            = 0.4444\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.6842\n",
      "  mmlu_loss                                              = 1.5489\n",
      "Profiling model for accuracies with 29 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [09:05<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l29w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.4437\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.1818\n",
      "  mmlu_eval_accuracy_anatomy                             =    0.5\n",
      "  mmlu_eval_accuracy_astronomy                           = 0.3125\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.5455\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.4138\n",
      "  mmlu_eval_accuracy_college_biology                     =  0.375\n",
      "  mmlu_eval_accuracy_college_chemistry                   =  0.375\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.2727\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.1818\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.3636\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.4545\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.3636\n",
      "  mmlu_eval_accuracy_conceptual_physics                  =    0.5\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.1667\n",
      "  mmlu_eval_accuracy_electrical_engineering              = 0.3125\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.3171\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.1429\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.3\n",
      "  mmlu_eval_accuracy_high_school_biology                 = 0.4375\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.3636\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.3333\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.6667\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.6818\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.5714\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.3953\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.2759\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3846\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.2353\n",
      "  mmlu_eval_accuracy_high_school_psychology              =   0.65\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.3913\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.6364\n",
      "  mmlu_eval_accuracy_high_school_world_history           = 0.5769\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.6957\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.4167\n",
      "  mmlu_eval_accuracy_international_law                   = 0.8462\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.3636\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.6111\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.2727\n",
      "  mmlu_eval_accuracy_management                          = 0.6364\n",
      "  mmlu_eval_accuracy_marketing                           =   0.76\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.7273\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.6395\n",
      "  mmlu_eval_accuracy_moral_disputes                      = 0.4737\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.24\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.6667\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.4412\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.3429\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.3226\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.2941\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.4194\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.4348\n",
      "  mmlu_eval_accuracy_public_relations                    = 0.3333\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.4815\n",
      "  mmlu_eval_accuracy_sociology                           = 0.6818\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.5455\n",
      "  mmlu_eval_accuracy_virology                            = 0.3333\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.6316\n",
      "  mmlu_loss                                              = 1.5953\n",
      "Profiling model for accuracies with 31 layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [09:42<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l31w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.4506\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.1818\n",
      "  mmlu_eval_accuracy_anatomy                             =    0.5\n",
      "  mmlu_eval_accuracy_astronomy                           =  0.375\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.4545\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.4483\n",
      "  mmlu_eval_accuracy_college_biology                     = 0.3125\n",
      "  mmlu_eval_accuracy_college_chemistry                   =  0.375\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.2727\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.3636\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.4091\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.3636\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.3636\n",
      "  mmlu_eval_accuracy_conceptual_physics                  = 0.4615\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.1667\n",
      "  mmlu_eval_accuracy_electrical_engineering              = 0.3125\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.3902\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.2143\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.2\n",
      "  mmlu_eval_accuracy_high_school_biology                 = 0.4688\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.4091\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.3333\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.7778\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.7273\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.5714\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.4419\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.2069\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3846\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.2941\n",
      "  mmlu_eval_accuracy_high_school_psychology              = 0.6667\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.3913\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.6364\n",
      "  mmlu_eval_accuracy_high_school_world_history           = 0.5769\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.6957\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.4167\n",
      "  mmlu_eval_accuracy_international_law                   = 0.7692\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.3636\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.6111\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.2727\n",
      "  mmlu_eval_accuracy_management                          = 0.6364\n",
      "  mmlu_eval_accuracy_marketing                           =   0.76\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.7273\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.6163\n",
      "  mmlu_eval_accuracy_moral_disputes                      =    0.5\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.24\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.6667\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.4412\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.3714\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.3226\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.3059\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.3871\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.4638\n",
      "  mmlu_eval_accuracy_public_relations                    = 0.3333\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.4815\n",
      "  mmlu_eval_accuracy_sociology                           = 0.6818\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.5455\n",
      "  mmlu_eval_accuracy_virology                            = 0.3889\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.6316\n",
      "  mmlu_loss                                              = 1.6391\n"
     ]
    }
   ],
   "source": [
    "all_metrics = {}\n",
    "\n",
    "data_module = make_data_module(tokenizer, args)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    **{k: v for k, v in data_module.items() if k != \"predict_dataset\"},\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "mmlu_dataset, abcd_idx, accuracy = get_mmlu_dataset_for_evaluation(args, tokenizer)\n",
    "args.eval_num_width = 1.0\n",
    "\n",
    "for num_layers in [17, 19, 21, 23, 25, 27, 29, 31]:\n",
    "    args.eval_num_layer = num_layers\n",
    "    print(f\"Profiling model for accuracies with {num_layers} layers\")\n",
    "    all_metrics = profile_accuracies(model, tokenizer, args, trainer, logger, mmlu_dataset, abcd_idx, accuracy, all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [07:33<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l24w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.4537\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.2727\n",
      "  mmlu_eval_accuracy_anatomy                             = 0.5714\n",
      "  mmlu_eval_accuracy_astronomy                           = 0.4375\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.2727\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.5172\n",
      "  mmlu_eval_accuracy_college_biology                     = 0.5625\n",
      "  mmlu_eval_accuracy_college_chemistry                   =    0.5\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.2727\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.2727\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.3636\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.4545\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.7273\n",
      "  mmlu_eval_accuracy_conceptual_physics                  = 0.4615\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.1667\n",
      "  mmlu_eval_accuracy_electrical_engineering              =   0.25\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.2195\n",
      "  mmlu_eval_accuracy_formal_logic                        = 0.3571\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.4\n",
      "  mmlu_eval_accuracy_high_school_biology                 = 0.4688\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.2727\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.4444\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.3333\n",
      "  mmlu_eval_accuracy_high_school_geography               = 0.8182\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics = 0.5714\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.3953\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.2759\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3462\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.2941\n",
      "  mmlu_eval_accuracy_high_school_psychology              = 0.6333\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.3913\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.6818\n",
      "  mmlu_eval_accuracy_high_school_world_history           =    0.5\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.5217\n",
      "  mmlu_eval_accuracy_human_sexuality                     = 0.3333\n",
      "  mmlu_eval_accuracy_international_law                   = 0.7692\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.2727\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.6111\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.3636\n",
      "  mmlu_eval_accuracy_management                          = 0.6364\n",
      "  mmlu_eval_accuracy_marketing                           =   0.72\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.7273\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.6047\n",
      "  mmlu_eval_accuracy_moral_disputes                      =    0.5\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.24\n",
      "  mmlu_eval_accuracy_nutrition                           =  0.697\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.4706\n",
      "  mmlu_eval_accuracy_prehistory                          = 0.3714\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.2258\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.2882\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.3548\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.4638\n",
      "  mmlu_eval_accuracy_public_relations                    = 0.4167\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.4444\n",
      "  mmlu_eval_accuracy_sociology                           = 0.7727\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.6364\n",
      "  mmlu_eval_accuracy_virology                            = 0.3333\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.5789\n",
      "  mmlu_loss                                              = 1.3652\n"
     ]
    }
   ],
   "source": [
    "# OLD\n",
    "args.eval_num_width = 1.0\n",
    "args.eval_num_layer = 24\n",
    "all_metrics = profile_accuracies(model, tokenizer, args, trainer, logger, mmlu_dataset, abcd_idx, accuracy, all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [06:20<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** mmlu_l20w1.0 metrics *****\n",
      "  mmlu_eval_accuracy                                     = 0.3624\n",
      "  mmlu_eval_accuracy_abstract_algebra                    = 0.2727\n",
      "  mmlu_eval_accuracy_anatomy                             =    0.5\n",
      "  mmlu_eval_accuracy_astronomy                           =   0.25\n",
      "  mmlu_eval_accuracy_business_ethics                     = 0.1818\n",
      "  mmlu_eval_accuracy_clinical_knowledge                  = 0.2759\n",
      "  mmlu_eval_accuracy_college_biology                     = 0.3125\n",
      "  mmlu_eval_accuracy_college_chemistry                   =  0.375\n",
      "  mmlu_eval_accuracy_college_computer_science            = 0.3636\n",
      "  mmlu_eval_accuracy_college_mathematics                 = 0.1818\n",
      "  mmlu_eval_accuracy_college_medicine                    = 0.3636\n",
      "  mmlu_eval_accuracy_college_physics                     = 0.6364\n",
      "  mmlu_eval_accuracy_computer_security                   = 0.2727\n",
      "  mmlu_eval_accuracy_conceptual_physics                  = 0.3846\n",
      "  mmlu_eval_accuracy_econometrics                        = 0.1667\n",
      "  mmlu_eval_accuracy_electrical_engineering              =  0.375\n",
      "  mmlu_eval_accuracy_elementary_mathematics              = 0.2927\n",
      "  mmlu_eval_accuracy_formal_logic                        =    0.5\n",
      "  mmlu_eval_accuracy_global_facts                        =    0.3\n",
      "  mmlu_eval_accuracy_high_school_biology                 = 0.4688\n",
      "  mmlu_eval_accuracy_high_school_chemistry               = 0.2727\n",
      "  mmlu_eval_accuracy_high_school_computer_science        = 0.2222\n",
      "  mmlu_eval_accuracy_high_school_european_history        = 0.2778\n",
      "  mmlu_eval_accuracy_high_school_geography               =    0.5\n",
      "  mmlu_eval_accuracy_high_school_government_and_politics =  0.381\n",
      "  mmlu_eval_accuracy_high_school_macroeconomics          = 0.4186\n",
      "  mmlu_eval_accuracy_high_school_mathematics             = 0.1724\n",
      "  mmlu_eval_accuracy_high_school_microeconomics          = 0.3077\n",
      "  mmlu_eval_accuracy_high_school_physics                 = 0.2353\n",
      "  mmlu_eval_accuracy_high_school_psychology              =    0.4\n",
      "  mmlu_eval_accuracy_high_school_statistics              = 0.3478\n",
      "  mmlu_eval_accuracy_high_school_us_history              = 0.4091\n",
      "  mmlu_eval_accuracy_high_school_world_history           = 0.4615\n",
      "  mmlu_eval_accuracy_human_aging                         = 0.3913\n",
      "  mmlu_eval_accuracy_human_sexuality                     =   0.25\n",
      "  mmlu_eval_accuracy_international_law                   = 0.6923\n",
      "  mmlu_eval_accuracy_jurisprudence                       = 0.2727\n",
      "  mmlu_eval_accuracy_logical_fallacies                   = 0.3889\n",
      "  mmlu_eval_accuracy_machine_learning                    = 0.0909\n",
      "  mmlu_eval_accuracy_management                          = 0.4545\n",
      "  mmlu_eval_accuracy_marketing                           =   0.52\n",
      "  mmlu_eval_accuracy_medical_genetics                    = 0.6364\n",
      "  mmlu_eval_accuracy_miscellaneous                       = 0.5233\n",
      "  mmlu_eval_accuracy_moral_disputes                      = 0.4474\n",
      "  mmlu_eval_accuracy_moral_scenarios                     =   0.29\n",
      "  mmlu_eval_accuracy_nutrition                           = 0.3939\n",
      "  mmlu_eval_accuracy_philosophy                          = 0.3824\n",
      "  mmlu_eval_accuracy_prehistory                          =    0.4\n",
      "  mmlu_eval_accuracy_professional_accounting             = 0.1935\n",
      "  mmlu_eval_accuracy_professional_law                    = 0.2588\n",
      "  mmlu_eval_accuracy_professional_medicine               = 0.2903\n",
      "  mmlu_eval_accuracy_professional_psychology             = 0.3913\n",
      "  mmlu_eval_accuracy_public_relations                    = 0.4167\n",
      "  mmlu_eval_accuracy_security_studies                    = 0.3704\n",
      "  mmlu_eval_accuracy_sociology                           = 0.4091\n",
      "  mmlu_eval_accuracy_us_foreign_policy                   = 0.4545\n",
      "  mmlu_eval_accuracy_virology                            = 0.3333\n",
      "  mmlu_eval_accuracy_world_religions                     = 0.5263\n",
      "  mmlu_loss                                              = 1.7521\n"
     ]
    }
   ],
   "source": [
    "args.eval_num_width = 1.0\n",
    "args.eval_num_layer = 20\n",
    "all_metrics = profile_accuracies(model, tokenizer, args, trainer, logger, mmlu_dataset, abcd_idx, accuracy, all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile Latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/serenity/scratch/dgarg/anaconda3/envs/amoaballm/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding correct special tokens to the llama tokenizer\n",
      "Loading adapters from checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    }
   ],
   "source": [
    "args = [\n",
    "    \"--model_name_or_path\", \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"--output_dir\", \"amoeba_llama2\",\n",
    "    \"--do_predict\", \"True\",\n",
    "    \"--do_eval\", \"False\",\n",
    "    \"--do_train\", \"False\",\n",
    "    \"--do_mmlu_eval\", \"False\",\n",
    "    \"--enable_shrinking\",\n",
    "    \"--min_num_layer\", \"20\",\n",
    "    \"--shrinking_method\", \"calib_dp\",\n",
    "    \"--shrinking_file\", \"dp_selection_strategy.npy\",\n",
    "    \"--shrinkable_width\",\n",
    "    \"--width_choice\", \"[1,7/8,3/4,5/8,1/2]\",\n",
    "    \"--prune_width_method\", \"flap\",\n",
    "    \"--use_moe_lora\",\n",
    "    \"--moe_num_expert\", \"5\",\n",
    "    \"--moe_topk\", \"2\",\n",
    "    \"--eval_num_layer\", \"32\",\n",
    "    \"--eval_num_width\", \"1\",\n",
    "    \"--predict_with_generate\", \"True\",\n",
    "]\n",
    "args, training_args = parse_args(args_list=args)\n",
    "checkpoint_dir = get_last_checkpoint(args.output_dir)\n",
    "model, tokenizer = load_model(args, checkpoint_dir)\n",
    "set_width_mask_and_bias(model, args)\n",
    "data_module = make_data_module(tokenizer=tokenizer, args=args)\n",
    "logger = logging.getLogger(__name__)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    **{k: v for k, v in data_module.items() if k != \"predict_dataset\"},\n",
    ")\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latency_stats(ttft, tbt, bs):\n",
    "    '''\n",
    "    ttft (time-to-first-token) is a dictionary with keys as batch_num and values as tuples (batch_size, latency)\n",
    "    tbt (time-between-tokens) is a dictionary with keys as batch_num and values as tuples (batch_size, num_tokens, avg_latency)\n",
    "\n",
    "    Returns:\n",
    "    - batch_size\n",
    "    - mean_ttft\n",
    "    - std_ttft\n",
    "    - mean_tbt\n",
    "    - std_tbt\n",
    "\n",
    "    Excludes the first 5 batches from both ttft and tbt\n",
    "    Excluded the last batch from ttft (since it is not a full batch)\n",
    "    - No need to exclude the last batch from tbt since the last batch is not included in tbt\n",
    "    '''\n",
    "    ttft_latencies = []\n",
    "    tbt_latencies = []\n",
    "    for (_, latency) in ttft.values():\n",
    "        ttft_latencies.append(latency * 1e6) # Convert to microseconds\n",
    "    \n",
    "    for (_, _, avg_latency) in tbt.values():\n",
    "        tbt_latencies.append(avg_latency * 1e6) # Convert to microseconds\n",
    "\n",
    "    ttft_latencies = ttft_latencies[5:-1]\n",
    "    tbt_latencies = tbt_latencies[5:]\n",
    "\n",
    "    return {\n",
    "        'batch_size': bs,\n",
    "        'mean_ttft': np.mean(ttft_latencies),\n",
    "        'std_ttft': np.std(ttft_latencies),\n",
    "        'mean_tbt': np.mean(tbt_latencies),\n",
    "        'std_tbt': np.std(tbt_latencies),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/serenity/scratch/dgarg/anaconda3/envs/amoaballm/lib/python3.9/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding correct special tokens to the llama tokenizer\n",
      "Loading adapters from checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: (1, 0.3030555248260498), 2: (1, 0.2775132656097412), 3: (1, 0.3207380771636963)}\n",
      "{1: (1, 999, 0.16963936401917054), 2: (1, 999, 0.1682439172590101)}\n",
      "{'predict_runtime': 506.7598, 'predict_samples_per_second': 0.006, 'predict_steps_per_second': 0.006}\n",
      "***** predict metrics *****\n",
      "  predict_runtime            = 0:08:26.75\n",
      "  predict_samples_per_second =      0.006\n",
      "  predict_steps_per_second   =      0.006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/serenity/scratch/dgarg/anaconda3/envs/amoaballm/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/serenity/scratch/dgarg/anaconda3/envs/amoaballm/lib/python3.9/site-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/serenity/scratch/dgarg/anaconda3/envs/amoaballm/lib/python3.9/site-packages/numpy/_core/_methods.py:218: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/serenity/scratch/dgarg/anaconda3/envs/amoaballm/lib/python3.9/site-packages/numpy/_core/_methods.py:175: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/serenity/scratch/dgarg/anaconda3/envs/amoaballm/lib/python3.9/site-packages/numpy/_core/_methods.py:210: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: (2, 0.31504154205322266), 2: (2, 0.3379056453704834), 3: (1, 0.28014683723449707)}\n",
      "{1: (2, 999, 0.26174528892333804), 2: (2, 999, 0.2621316623401355)}\n",
      "{'predict_runtime': 692.9456, 'predict_samples_per_second': 0.007, 'predict_steps_per_second': 0.004}\n",
      "***** predict metrics *****\n",
      "  predict_runtime            = 0:11:32.94\n",
      "  predict_samples_per_second =      0.007\n",
      "  predict_steps_per_second   =      0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/serenity/scratch/dgarg/anaconda3/envs/amoaballm/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/serenity/scratch/dgarg/anaconda3/envs/amoaballm/lib/python3.9/site-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/serenity/scratch/dgarg/anaconda3/envs/amoaballm/lib/python3.9/site-packages/numpy/_core/_methods.py:218: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/serenity/scratch/dgarg/anaconda3/envs/amoaballm/lib/python3.9/site-packages/numpy/_core/_methods.py:175: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/serenity/scratch/dgarg/anaconda3/envs/amoaballm/lib/python3.9/site-packages/numpy/_core/_methods.py:210: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "args = [\n",
    "    \"--model_name_or_path\", \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"--output_dir\", \"amoeba_llama2\",\n",
    "    \"--do_predict\", \"True\",\n",
    "    \"--do_eval\", \"False\",\n",
    "    \"--do_train\", \"False\",\n",
    "    \"--do_mmlu_eval\", \"False\",\n",
    "    \"--enable_shrinking\",\n",
    "    \"--min_num_layer\", \"20\",\n",
    "    \"--shrinking_method\", \"calib_dp\",\n",
    "    \"--shrinking_file\", \"dp_selection_strategy.npy\",\n",
    "    \"--shrinkable_width\",\n",
    "    \"--width_choice\", \"[1,7/8,3/4,5/8,1/2]\",\n",
    "    \"--prune_width_method\", \"flap\",\n",
    "    \"--use_moe_lora\",\n",
    "    \"--moe_num_expert\", \"5\",\n",
    "    \"--moe_topk\", \"2\",\n",
    "    \"--eval_num_layer\", \"32\",\n",
    "    \"--eval_num_width\", \"1\",\n",
    "    \"--predict_with_generate\", \"True\",\n",
    "    \"--max_new_tokens\", \"1000\",\n",
    "]\n",
    "args, training_args = parse_args(args_list=args)\n",
    "checkpoint_dir = get_last_checkpoint(args.output_dir)\n",
    "model, tokenizer = load_model(args, checkpoint_dir)\n",
    "set_width_mask_and_bias(model, args)\n",
    "\n",
    "args.eval_num_width = 1.0\n",
    "args.eval_num_layer = 32\n",
    "key = f'l{args.eval_num_layer}w{args.eval_num_width}'\n",
    "results[key] = []\n",
    "\n",
    "for bs in [1, 2]:\n",
    "    training_args.per_device_eval_batch_size = bs\n",
    "    args.eval_dataset_size = training_args.per_device_eval_batch_size * 2 + 1\n",
    "    data_module = make_data_module(tokenizer=tokenizer, args=args)\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        **{k: v for k, v in data_module.items() if k != \"predict_dataset\"},\n",
    "    )\n",
    "    ttft, tbt = profile_latencies(model, tokenizer, args, logger, trainer, data_module)\n",
    "    results[key].append(get_latency_stats(ttft, tbt, bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 51/51 [00:00<00:00, 3375.99 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: (1, 0.3038644790649414), 2: (1, 0.2741265296936035), 3: (1, 0.28331518173217773), 4: (1, 0.2780489921569824), 5: (1, 0.2807955741882324), 6: (1, 0.2796006202697754), 7: (1, 0.2781693935394287), 8: (1, 0.27220702171325684), 9: (1, 0.27780890464782715), 10: (1, 0.2789013385772705), 11: (1, 0.2794163227081299), 12: (1, 0.2766284942626953), 13: (1, 0.27938079833984375), 14: (1, 0.2822425365447998), 15: (1, 0.27423715591430664), 16: (1, 0.27529263496398926), 17: (1, 0.27825307846069336), 18: (1, 0.2731161117553711), 19: (1, 0.2792332172393799), 20: (1, 0.27240848541259766), 21: (1, 0.27353858947753906), 22: (1, 0.2762627601623535), 23: (1, 0.2716386318206787), 24: (1, 0.2760813236236572), 25: (1, 0.27935123443603516), 26: (1, 0.28093767166137695), 27: (1, 0.27518415451049805), 28: (1, 0.2740812301635742), 29: (1, 0.2783832550048828), 30: (1, 0.2757689952850342), 31: (1, 0.2825438976287842), 32: (1, 0.278536319732666), 33: (1, 0.27353692054748535), 34: (1, 0.27033424377441406), 35: (1, 0.28322863578796387), 36: (1, 0.27285265922546387), 37: (1, 0.27390146255493164), 38: (1, 0.2739717960357666), 39: (1, 0.27931833267211914), 40: (1, 0.27176976203918457), 41: (1, 0.2749001979827881), 42: (1, 0.2757244110107422), 43: (1, 0.2784297466278076), 44: (1, 0.2725503444671631), 45: (1, 0.27316856384277344), 46: (1, 0.2754666805267334), 47: (1, 0.27506136894226074), 48: (1, 0.2771923542022705), 49: (1, 0.2751467227935791), 50: (1, 0.28182554244995117), 51: (1, 0.27018237113952637)}\n",
      "{1: (1, 255, 0.16671867931590362), 2: (1, 255, 0.16656443745482202), 3: (1, 255, 0.166414810629452), 4: (1, 255, 0.1664179988935882), 5: (1, 255, 0.16707810701108447), 6: (1, 255, 0.16660382607403923), 7: (1, 255, 0.16660737243353152), 8: (1, 255, 0.16661927185806574), 9: (1, 255, 0.1665192678862927), 10: (1, 255, 0.16654762193268421), 11: (1, 255, 0.16652691785027), 12: (1, 255, 0.16627609309028177), 13: (1, 255, 0.16621648844550638), 14: (1, 255, 0.16629108447654575), 15: (1, 255, 0.16632963535832423), 16: (1, 255, 0.16636919788285798), 17: (1, 255, 0.1662341454449822), 18: (1, 255, 0.16627515811546176), 19: (1, 255, 0.1664033244637882), 20: (1, 255, 0.1662882973166073), 21: (1, 255, 0.16643901806251676), 22: (1, 255, 0.16642223058962355), 23: (1, 255, 0.16627958054636038), 24: (1, 255, 0.1661767239664115), 25: (1, 255, 0.16618119314605115), 26: (1, 255, 0.16602127692278693), 27: (1, 255, 0.16600824804867015), 28: (1, 255, 0.16666705468121698), 29: (1, 255, 0.16647442368900076), 30: (1, 255, 0.16665648759580126), 31: (1, 255, 0.16671586410672057), 32: (1, 255, 0.16660502751668294), 33: (1, 255, 0.16662311460457596), 34: (1, 255, 0.16662433848661534), 35: (1, 255, 0.1666752955492805), 36: (1, 255, 0.1667463040819355), 37: (1, 255, 0.16656975746154784), 38: (1, 255, 0.16671165578505573), 39: (1, 255, 0.16668465838712804), 40: (1, 255, 0.16669424468395758), 41: (1, 255, 0.16667400808895336), 42: (1, 255, 0.16669739367915135), 43: (1, 255, 0.16656024315777948), 44: (1, 255, 0.16661655108133952), 45: (1, 255, 0.16655754014557483), 46: (1, 255, 0.16659323280932856), 47: (1, 255, 0.16654725542255477), 48: (1, 255, 0.1666969243217917), 49: (1, 255, 0.16589328541475185), 50: (1, 255, 0.16591440836588542)}\n",
      "{'predict_runtime': 2179.0769, 'predict_samples_per_second': 0.023, 'predict_steps_per_second': 0.023}\n",
      "***** predict metrics *****\n",
      "  predict_runtime            = 0:36:19.07\n",
      "  predict_samples_per_second =      0.023\n",
      "  predict_steps_per_second   =      0.023\n",
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 101/101 [00:00<00:00, 4720.16 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: (2, 0.3126344680786133), 2: (2, 0.3053748607635498), 3: (2, 0.30750584602355957), 4: (2, 0.3082563877105713), 5: (2, 0.3021209239959717), 6: (2, 0.3059115409851074), 7: (2, 0.30779218673706055), 8: (2, 0.2835876941680908), 9: (2, 0.3032567501068115), 10: (2, 0.3028373718261719), 11: (2, 0.2835381031036377), 12: (2, 0.2874476909637451), 13: (2, 0.30619359016418457), 14: (2, 0.2849307060241699), 15: (2, 0.3063216209411621), 16: (2, 0.3050241470336914), 17: (2, 0.3381016254425049), 18: (2, 0.30101704597473145), 19: (2, 0.2843360900878906), 20: (2, 0.3032817840576172), 21: (2, 0.28408336639404297), 22: (2, 0.3056328296661377), 23: (2, 0.2859523296356201), 24: (2, 0.30654168128967285), 25: (2, 0.35906529426574707), 26: (2, 0.2900428771972656), 27: (2, 0.3070697784423828), 28: (2, 0.2883486747741699), 29: (2, 0.303342342376709), 30: (2, 0.31267499923706055), 31: (2, 0.3031439781188965), 32: (2, 0.3619270324707031), 33: (2, 0.35699462890625), 34: (2, 0.3088827133178711), 35: (2, 0.34098291397094727), 36: (2, 0.2823169231414795), 37: (2, 0.3247714042663574), 38: (2, 0.30063438415527344), 39: (2, 0.3035924434661865), 40: (2, 0.2863295078277588), 41: (2, 0.28528475761413574), 42: (2, 0.3077106475830078), 43: (2, 0.30898165702819824), 44: (2, 0.3388712406158447), 45: (2, 0.2843046188354492), 46: (2, 0.31394267082214355), 47: (2, 0.30891990661621094), 48: (2, 0.30717968940734863), 49: (2, 0.2874143123626709), 50: (2, 0.30693936347961426), 51: (1, 0.27027344703674316)}\n",
      "{1: (2, 255, 0.256981570112939), 2: (2, 255, 0.25659047856050377), 3: (2, 255, 0.2565861720664828), 4: (2, 255, 0.25665625590904084), 5: (2, 255, 0.2576565789241417), 6: (2, 255, 0.2574072379691928), 7: (2, 255, 0.25725475479574766), 8: (2, 255, 0.2573122080634622), 9: (2, 255, 0.2570429007212321), 10: (2, 255, 0.25718456156113567), 11: (2, 255, 0.2571098346336215), 12: (2, 255, 0.2571837331734452), 13: (2, 255, 0.25725395258735206), 14: (2, 255, 0.25729804039001464), 15: (2, 255, 0.2566230110093659), 16: (2, 255, 0.2567782261792351), 17: (2, 255, 0.2571282283932555), 18: (2, 255, 0.25663051511727125), 19: (2, 255, 0.25648356605978573), 20: (2, 255, 0.2563133006002389), 21: (2, 255, 0.2563582813038546), 22: (2, 255, 0.25644659435047823), 23: (2, 255, 0.25646636345807244), 24: (2, 255, 0.25613565257951326), 25: (2, 255, 0.2563915579926734), 26: (2, 255, 0.2563835471284156), 27: (2, 255, 0.2563912157918893), 28: (2, 255, 0.2580400513667686), 29: (2, 255, 0.25744138044469494), 30: (2, 255, 0.25709603066537895), 31: (2, 255, 0.2561366034489052), 32: (2, 255, 0.25807203124551215), 33: (2, 255, 0.2568556355495079), 34: (2, 255, 0.25615151536230946), 35: (2, 255, 0.2577662701700248), 36: (2, 255, 0.2575570087806851), 37: (2, 255, 0.2568457519306856), 38: (2, 255, 0.25925155340456496), 39: (2, 255, 0.2576731401331284), 40: (2, 255, 0.25976608968248555), 41: (2, 255, 0.2558754444122314), 42: (2, 255, 0.2558285385954614), 43: (2, 255, 0.25562743953630035), 44: (2, 255, 0.2555605205835081), 45: (2, 255, 0.25624151884340773), 46: (2, 255, 0.25623729556214575), 47: (2, 255, 0.25650346419390513), 48: (2, 255, 0.2564562853644876), 49: (2, 255, 0.256367826461792), 50: (2, 255, 0.25660819352841846)}\n",
      "{'predict_runtime': 3333.3043, 'predict_samples_per_second': 0.03, 'predict_steps_per_second': 0.015}\n",
      "***** predict metrics *****\n",
      "  predict_runtime            = 0:55:33.30\n",
      "  predict_samples_per_second =       0.03\n",
      "  predict_steps_per_second   =      0.015\n",
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 201/201 [00:00<00:00, 6668.53 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: (4, 0.40064024925231934), 2: (4, 0.351942777633667), 3: (4, 0.3502521514892578), 4: (4, 0.37799739837646484), 5: (4, 0.400831937789917), 6: (4, 0.3632385730743408), 7: (4, 0.4083218574523926), 8: (4, 0.4125967025756836), 9: (4, 0.3462333679199219), 10: (4, 0.37140798568725586), 11: (4, 0.3531012535095215), 12: (4, 0.3996915817260742), 13: (4, 0.4082827568054199), 14: (4, 0.4032111167907715), 15: (4, 0.3596360683441162), 16: (4, 0.39844799041748047), 17: (4, 0.4551057815551758), 18: (4, 0.3063664436340332), 19: (4, 0.4016690254211426), 20: (4, 0.3898580074310303), 21: (4, 0.350477933883667), 22: (4, 0.4286203384399414), 23: (4, 0.4322471618652344), 24: (4, 0.4076857566833496), 25: (4, 0.41359949111938477), 26: (4, 0.36310696601867676), 27: (4, 0.40729784965515137), 28: (4, 0.3481464385986328), 29: (4, 0.37674427032470703), 30: (4, 0.42316102981567383), 31: (4, 0.39426612854003906), 32: (4, 0.35883021354675293), 33: (4, 0.4020252227783203), 34: (4, 0.35908031463623047), 35: (4, 0.37469935417175293), 36: (4, 0.4033348560333252), 37: (4, 0.3985598087310791), 38: (4, 0.4123408794403076), 39: (4, 0.36014342308044434), 40: (4, 0.4108741283416748), 41: (4, 0.4820835590362549), 42: (4, 0.4008162021636963), 43: (4, 0.31751132011413574), 44: (4, 0.35192203521728516), 45: (4, 0.4419059753417969), 46: (4, 0.3683593273162842), 47: (4, 0.35586047172546387), 48: (4, 0.41290879249572754), 49: (4, 0.4097888469696045), 50: (4, 0.3596367835998535), 51: (1, 0.2702929973602295)}\n",
      "{1: (4, 255, 0.2579668867821787), 2: (4, 255, 0.2579104348724964), 3: (4, 255, 0.257575771855373), 4: (4, 255, 0.2574545074911678), 5: (4, 255, 0.257159797818053), 6: (4, 255, 0.2569685515235452), 7: (4, 255, 0.2578230568006927), 8: (4, 255, 0.25744575893177707), 9: (4, 255, 0.2571349499272365), 10: (4, 255, 0.25727301859388163), 11: (4, 255, 0.2569385893204633), 12: (4, 255, 0.2568244307648902), 13: (4, 255, 0.25698158694248574), 14: (4, 255, 0.2571775810391295), 15: (4, 255, 0.25711895344304103), 16: (4, 255, 0.2570401855543548), 17: (4, 255, 0.25948794589323154), 18: (4, 255, 0.2566485376919017), 19: (4, 255, 0.2569146137611539), 20: (4, 255, 0.2568761105630912), 21: (4, 255, 0.25693829293344533), 22: (4, 255, 0.25724687015309056), 23: (4, 255, 0.2572429703731163), 24: (4, 255, 0.25690912639393526), 25: (4, 255, 0.2570059065725289), 26: (4, 255, 0.2567265080470665), 27: (4, 255, 0.2569931974598006), 28: (4, 255, 0.25696654787250595), 29: (4, 255, 0.25693104874853995), 30: (4, 255, 0.25712335904439293), 31: (4, 255, 0.2569808735566981), 32: (4, 255, 0.25698366726146027), 33: (4, 255, 0.25704563084770654), 34: (4, 255, 0.2570845491745893), 35: (4, 255, 0.25691268116820093), 36: (4, 255, 0.2566896644293093), 37: (4, 255, 0.2567989237168256), 38: (4, 255, 0.2578049061345119), 39: (4, 255, 0.25697558627409095), 40: (4, 255, 0.25715179443359376), 41: (4, 255, 0.2590614216000426), 42: (4, 255, 0.2571055561888452), 43: (4, 255, 0.2570254297817455), 44: (4, 255, 0.2569696482490091), 45: (4, 255, 0.25833667025846596), 46: (4, 255, 0.2574574012382358), 47: (4, 255, 0.2571441444696165), 48: (4, 255, 0.2573889956754797), 49: (4, 255, 0.2574869408326991), 50: (4, 255, 0.2573681597616158)}\n",
      "{'predict_runtime': 3341.969, 'predict_samples_per_second': 0.06, 'predict_steps_per_second': 0.015}\n",
      "***** predict metrics *****\n",
      "  predict_runtime            = 0:55:41.96\n",
      "  predict_samples_per_second =       0.06\n",
      "  predict_steps_per_second   =      0.015\n"
     ]
    }
   ],
   "source": [
    "args.eval_num_width = 1.0\n",
    "args.eval_num_layer = 32\n",
    "key = f'l{args.eval_num_layer}w{args.eval_num_width}'\n",
    "results[key] = []\n",
    "\n",
    "for bs in [1, 2, 4]:\n",
    "    training_args.per_device_eval_batch_size = bs\n",
    "    args.eval_dataset_size = training_args.per_device_eval_batch_size * 50 + 1\n",
    "    data_module = make_data_module(tokenizer=tokenizer, args=args)\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        **{k: v for k, v in data_module.items() if k != \"predict_dataset\"},\n",
    "    )\n",
    "    ttft, tbt = profile_latencies(model, tokenizer, args, logger, trainer, data_module)\n",
    "    results[key].append(get_latency_stats(ttft, tbt, bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'latency_results_{key}.json', 'w') as fout:\n",
    "    json.dump(results, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: (1, 0.17556190490722656), 2: (1, 0.15228557586669922), 3: (1, 0.1572272777557373), 4: (1, 0.15154170989990234), 5: (1, 0.15828680992126465), 6: (1, 0.1560378074645996), 7: (1, 0.1594240665435791), 8: (1, 0.1539771556854248), 9: (1, 0.15257596969604492), 10: (1, 0.15273761749267578), 11: (1, 0.15189528465270996), 12: (1, 0.15955734252929688), 13: (1, 0.15547418594360352), 14: (1, 0.15855121612548828), 15: (1, 0.15370392799377441), 16: (1, 0.15348434448242188), 17: (1, 0.15083909034729004), 18: (1, 0.1517035961151123), 19: (1, 0.15773892402648926), 20: (1, 0.15506720542907715), 21: (1, 0.14890575408935547), 22: (1, 0.15526080131530762), 23: (1, 0.14904499053955078), 24: (1, 0.15063929557800293), 25: (1, 0.15562224388122559), 26: (1, 0.15830183029174805), 27: (1, 0.1479654312133789), 28: (1, 0.1543736457824707), 29: (1, 0.1522536277770996), 30: (1, 0.15025734901428223), 31: (1, 0.15725207328796387), 32: (1, 0.15392422676086426), 33: (1, 0.15011382102966309), 34: (1, 0.14950323104858398), 35: (1, 0.15928983688354492), 36: (1, 0.15423846244812012), 37: (1, 0.15260601043701172), 38: (1, 0.1492156982421875), 39: (1, 0.1577150821685791), 40: (1, 0.15334105491638184), 41: (1, 0.15320801734924316), 42: (1, 0.14921998977661133), 43: (1, 0.1530132293701172), 44: (1, 0.15614819526672363), 45: (1, 0.15243315696716309), 46: (1, 0.14952635765075684), 47: (1, 0.15598011016845703), 48: (1, 0.15708255767822266), 49: (1, 0.15174078941345215), 50: (1, 0.15886712074279785), 51: (1, 0.1535782814025879)}\n",
      "{1: (1, 255, 0.0902320197984284), 2: (1, 255, 0.08996496387556488), 3: (1, 255, 0.08998204493055156), 4: (1, 255, 0.09025332039477778), 5: (1, 255, 0.08987710055182963), 6: (1, 255, 0.08986463733747893), 7: (1, 255, 0.08994082469566196), 8: (1, 255, 0.0899165667739569), 9: (1, 255, 0.09009882983039408), 10: (1, 255, 0.09009531526004567), 11: (1, 255, 0.09021971552979713), 12: (1, 255, 0.09021917978922527), 13: (1, 255, 0.09003500938415528), 14: (1, 255, 0.09011743769926184), 15: (1, 255, 0.09029810755860572), 16: (1, 255, 0.09025713789696786), 17: (1, 255, 0.09037066347458783), 18: (1, 255, 0.09023815229827283), 19: (1, 255, 0.09014273531296674), 20: (1, 255, 0.0900535359102137), 21: (1, 255, 0.09010972883187088), 22: (1, 255, 0.09023575689278397), 23: (1, 255, 0.09009381930033365), 24: (1, 255, 0.09015913664125928), 25: (1, 255, 0.09010593096415202), 26: (1, 255, 0.09004928177478266), 27: (1, 255, 0.09001415196587058), 28: (1, 255, 0.08997209118861778), 29: (1, 255, 0.09007901490903368), 30: (1, 255, 0.09006840948965035), 31: (1, 255, 0.09009930012272853), 32: (1, 255, 0.09005736743702608), 33: (1, 255, 0.09002007970622941), 34: (1, 255, 0.09001718969906078), 35: (1, 255, 0.09001802556654986), 36: (1, 255, 0.09002878619175331), 37: (1, 255, 0.09010450793247597), 38: (1, 255, 0.08999819381564271), 39: (1, 255, 0.09007901771395814), 40: (1, 255, 0.09002634029762417), 41: (1, 255, 0.09014922590816722), 42: (1, 255, 0.0901063320683498), 43: (1, 255, 0.09003690738303989), 44: (1, 255, 0.09001551141925887), 45: (1, 255, 0.09015747332105449), 46: (1, 255, 0.09004868900074678), 47: (1, 255, 0.09000372045180377), 48: (1, 255, 0.09007150051640529), 49: (1, 255, 0.08999836304608513), 50: (1, 255, 0.08998764168982412)}\n",
      "{'predict_runtime': 1179.387, 'predict_samples_per_second': 0.043, 'predict_steps_per_second': 0.043}\n",
      "***** predict metrics *****\n",
      "  predict_runtime            = 0:19:39.38\n",
      "  predict_samples_per_second =      0.043\n",
      "  predict_steps_per_second   =      0.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: (2, 0.18354010581970215), 2: (2, 0.17166686058044434), 3: (2, 0.1948845386505127), 4: (2, 0.2041797637939453), 5: (2, 0.18320178985595703), 6: (2, 0.17308735847473145), 7: (2, 0.17366695404052734), 8: (2, 0.1934971809387207), 9: (2, 0.16674470901489258), 10: (2, 0.1659224033355713), 11: (2, 0.15595769882202148), 12: (2, 0.1587052345275879), 13: (2, 0.16535401344299316), 14: (2, 0.155717134475708), 15: (2, 0.16790342330932617), 16: (2, 0.1683814525604248), 17: (2, 0.16209793090820312), 18: (2, 0.17130756378173828), 19: (2, 0.15598797798156738), 20: (2, 0.17201614379882812), 21: (2, 0.15668439865112305), 22: (2, 0.1759932041168213), 23: (2, 0.16348791122436523), 24: (2, 0.1669144630432129), 25: (2, 0.16908478736877441), 26: (2, 0.16192889213562012), 27: (2, 0.16704988479614258), 28: (2, 0.1731879711151123), 29: (2, 0.17009282112121582), 30: (2, 0.16892027854919434), 31: (2, 0.17957139015197754), 32: (2, 0.1902472972869873), 33: (2, 0.1934669017791748), 34: (2, 0.203110933303833), 35: (2, 0.15724492073059082), 36: (2, 0.1565103530883789), 37: (2, 0.2033519744873047), 38: (2, 0.1660752296447754), 39: (2, 0.192854642868042), 40: (2, 0.15772247314453125), 41: (2, 0.1912693977355957), 42: (2, 0.2062697410583496), 43: (2, 0.17284607887268066), 44: (2, 0.1657240390777588), 45: (2, 0.16170024871826172), 46: (2, 0.2066032886505127), 47: (2, 0.17101788520812988), 48: (2, 0.16697001457214355), 49: (2, 0.16593098640441895), 50: (2, 0.1682734489440918), 51: (1, 0.1904430389404297)}\n",
      "{1: (2, 255, 0.13855202057782343), 2: (2, 255, 0.13829895094329236), 3: (2, 255, 0.1384600845037722), 4: (2, 255, 0.13844337743871352), 5: (2, 255, 0.13886771763072295), 6: (2, 255, 0.13837187897925282), 7: (2, 255, 0.13836794554018506), 8: (2, 255, 0.1384004602245256), 9: (2, 255, 0.13843195391636268), 10: (2, 255, 0.1384629445917466), 11: (2, 255, 0.1384662534676346), 12: (2, 255, 0.13850986162821452), 13: (2, 255, 0.13823327550701067), 14: (2, 255, 0.13831440607706705), 15: (2, 255, 0.13824846043306238), 16: (2, 255, 0.13835176019107595), 17: (2, 255, 0.13821512671077954), 18: (2, 255, 0.13829069418065687), 19: (2, 255, 0.13826177353952446), 20: (2, 255, 0.1383764949499392), 21: (2, 255, 0.13824298241559196), 22: (2, 255, 0.13834265727622835), 23: (2, 255, 0.13835539163327684), 24: (2, 255, 0.13848931742649453), 25: (2, 255, 0.13834685063829608), 26: (2, 255, 0.1384400676278507), 27: (2, 255, 0.13838988191941204), 28: (2, 255, 0.13837321599324545), 29: (2, 255, 0.13838335860009288), 30: (2, 255, 0.13827001253763835), 31: (2, 255, 0.1381434206869088), 32: (2, 255, 0.13827899951560824), 33: (2, 255, 0.13822945987477023), 34: (2, 255, 0.13829703985476027), 35: (2, 255, 0.13818076825609393), 36: (2, 255, 0.1380690004311356), 37: (2, 255, 0.13820963747361126), 38: (2, 255, 0.13812075222239775), 39: (2, 255, 0.13825132893581016), 40: (2, 255, 0.13807356591318168), 41: (2, 255, 0.13807653539321002), 42: (2, 255, 0.13821107546488445), 43: (2, 255, 0.1381544636745079), 44: (2, 255, 0.1380791421030082), 45: (2, 255, 0.13807362014172123), 46: (2, 255, 0.13806313626906452), 47: (2, 255, 0.13839232687856637), 48: (2, 255, 0.13818284296521954), 49: (2, 255, 0.13821802045784745), 50: (2, 255, 0.13805702527364094)}\n",
      "{'predict_runtime': 1795.1694, 'predict_samples_per_second': 0.056, 'predict_steps_per_second': 0.028}\n",
      "***** predict metrics *****\n",
      "  predict_runtime            = 0:29:55.16\n",
      "  predict_samples_per_second =      0.056\n",
      "  predict_steps_per_second   =      0.028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: (4, 0.23214340209960938), 2: (4, 0.1932201385498047), 3: (4, 0.19307541847229004), 4: (4, 0.21932101249694824), 5: (4, 0.1948084831237793), 6: (4, 0.17537283897399902), 7: (4, 0.20030713081359863), 8: (4, 0.21202468872070312), 9: (4, 0.21373271942138672), 10: (4, 0.1935889720916748), 11: (4, 0.19538116455078125), 12: (4, 0.1917726993560791), 13: (4, 0.19446492195129395), 14: (4, 0.20087623596191406), 15: (4, 0.19704914093017578), 16: (4, 0.1940138339996338), 17: (4, 0.2677597999572754), 18: (4, 0.17223882675170898), 19: (4, 0.19216609001159668), 20: (4, 0.22747206687927246), 21: (4, 0.1938340663909912), 22: (4, 0.20758914947509766), 23: (4, 0.24570941925048828), 24: (4, 0.23257780075073242), 25: (4, 0.23537325859069824), 26: (4, 0.20461583137512207), 27: (4, 0.19701933860778809), 28: (4, 0.19451212882995605), 29: (4, 0.2356712818145752), 30: (4, 0.2338881492614746), 31: (4, 0.19288349151611328), 32: (4, 0.19669890403747559), 33: (4, 0.20063471794128418), 34: (4, 0.19693613052368164), 35: (4, 0.23235321044921875), 36: (4, 0.22132039070129395), 37: (4, 0.19202136993408203), 38: (4, 0.20213747024536133), 39: (4, 0.19737625122070312), 40: (4, 0.2003488540649414), 41: (4, 0.19716334342956543), 42: (4, 0.19346165657043457), 43: (4, 0.1691889762878418), 44: (4, 0.1728682518005371), 45: (4, 0.22861409187316895), 46: (4, 0.19892525672912598), 47: (4, 0.2077169418334961), 48: (4, 0.19971466064453125), 49: (4, 0.2369396686553955), 50: (4, 0.1901264190673828), 51: (1, 0.15680241584777832)}\n",
      "{1: (4, 255, 0.13912516388238644), 2: (4, 255, 0.13899116703108244), 3: (4, 255, 0.13899791941923254), 4: (4, 255, 0.13917638180302638), 5: (4, 255, 0.13893394563712325), 6: (4, 255, 0.13891573232762955), 7: (4, 255, 0.1390462370479808), 8: (4, 255, 0.13916776601006003), 9: (4, 255, 0.13894762618868958), 10: (4, 255, 0.13905194413428212), 11: (4, 255, 0.13912881869895785), 12: (4, 255, 0.1390915730420281), 13: (4, 255, 0.1391032807967242), 14: (4, 255, 0.13904755723242665), 15: (4, 255, 0.13909671072866403), 16: (4, 255, 0.13898746079089594), 17: (4, 255, 0.1396496576421401), 18: (4, 255, 0.13925561717912263), 19: (4, 255, 0.1389693765079274), 20: (4, 255, 0.13898476899838916), 21: (4, 255, 0.1390663745356541), 22: (4, 255, 0.13907901446024576), 23: (4, 255, 0.13910855405470904), 24: (4, 255, 0.13898531595865884), 25: (4, 255, 0.138949888827754), 26: (4, 255, 0.14001728693644205), 27: (4, 255, 0.13901518840415805), 28: (4, 255, 0.1389636993408203), 29: (4, 255, 0.13898953643499637), 30: (4, 255, 0.13921826867496265), 31: (4, 255, 0.13913152451608696), 32: (4, 255, 0.1391475845785702), 33: (4, 255, 0.1402382635602764), 34: (4, 255, 0.13901950331295237), 35: (4, 255, 0.13900270742528578), 36: (4, 255, 0.13905779520670572), 37: (4, 255, 0.13913895476098154), 38: (4, 255, 0.13924820937362373), 39: (4, 255, 0.13915805068670534), 40: (4, 255, 0.13920273406832825), 41: (4, 255, 0.13921661750943054), 42: (4, 255, 0.13916841207766065), 43: (4, 255, 0.13908942633984137), 44: (4, 255, 0.1390493168550379), 45: (4, 255, 0.1396656821755802), 46: (4, 255, 0.13905133640064912), 47: (4, 255, 0.13898449037589278), 48: (4, 255, 0.1390462538775276), 49: (4, 255, 0.13901487144769406), 50: (4, 255, 0.13906865680918973)}\n",
      "{'predict_runtime': 1807.4522, 'predict_samples_per_second': 0.111, 'predict_steps_per_second': 0.028}\n",
      "***** predict metrics *****\n",
      "  predict_runtime            = 0:30:07.45\n",
      "  predict_samples_per_second =      0.111\n",
      "  predict_steps_per_second   =      0.028\n"
     ]
    }
   ],
   "source": [
    "args.eval_num_width = 1.0\n",
    "args.eval_num_layer = 17\n",
    "key = f'l{args.eval_num_layer}w{args.eval_num_width}'\n",
    "results[key] = []\n",
    "\n",
    "for bs in [1, 2, 4]:\n",
    "    training_args.per_device_eval_batch_size = bs\n",
    "    args.eval_dataset_size = training_args.per_device_eval_batch_size * 50 + 1\n",
    "    data_module = make_data_module(tokenizer=tokenizer, args=args)\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        **{k: v for k, v in data_module.items() if k != \"predict_dataset\"},\n",
    "    )\n",
    "    ttft, tbt = profile_latencies(model, tokenizer, args, logger, trainer, data_module)\n",
    "    results[key].append(get_latency_stats(ttft, tbt, bs))\n",
    "\n",
    "with open(f'latency_results_{key}.json', 'w') as fout:\n",
    "    json.dump(results, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: (2, 0.19472432136535645), 2: (2, 0.23932957649230957), 3: (2, 0.2298414707183838), 4: (2, 0.19170069694519043), 5: (2, 0.23890280723571777), 6: (2, 0.1951150894165039), 7: (2, 0.19726133346557617), 8: (2, 0.19946551322937012), 9: (2, 0.22259259223937988), 10: (2, 0.1975388526916504), 11: (2, 0.183868408203125), 12: (2, 0.18013644218444824), 13: (2, 0.20206952095031738), 14: (2, 0.18253016471862793), 15: (2, 0.19663333892822266), 16: (2, 0.20292949676513672), 17: (2, 0.18525362014770508)}\n",
      "{1: (2, 255, 0.16180359989989038), 2: (2, 255, 0.16154881926143871), 3: (2, 255, 0.16172155679440967), 4: (2, 255, 0.16161078565260945), 5: (2, 255, 0.16172690765530456), 6: (2, 255, 0.16171090275633568), 7: (2, 255, 0.16164220641641056), 8: (2, 255, 0.16167252858479816), 9: (2, 255, 0.16176543422773773), 10: (2, 255, 0.16182504635231168), 11: (2, 255, 0.16167325225530887), 12: (2, 255, 0.1619705181495816), 13: (2, 255, 0.16178720324647192), 14: (2, 255, 0.16169480155496035), 15: (2, 255, 0.1616290288812974), 16: (2, 255, 0.1617861972135656)}\n",
      "{'predict_runtime': 704.5345, 'predict_samples_per_second': 0.048, 'predict_steps_per_second': 0.024}\n",
      "***** predict metrics *****\n",
      "  predict_runtime            = 0:11:44.53\n",
      "  predict_samples_per_second =      0.048\n",
      "  predict_steps_per_second   =      0.024\n"
     ]
    }
   ],
   "source": [
    "training_args.per_device_eval_batch_size = 2\n",
    "args.eval_num_width = 1.0\n",
    "args.eval_num_layer = 20\n",
    "profile_latencies(model, tokenizer, args, logger, trainer, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "[\"Once upon a time, a young boy named Max dreams of becoming an engineer, to make his world a better place. everybody thinks he's too young to do that, but he persists. One day, he meets a robot, and the world becomes a whole new adventure.\\nA 4th- grade boy learns how to use a computer and create an animation with his friends.\"]\n"
     ]
    }
   ],
   "source": [
    "type(model)\n",
    "print(args.bits)\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "print(inputs.keys())\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs.get(\"attention_mask\"),\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling\n",
    "1. For batchsizes 1, 2, 4, and possibly 8\n",
    "2. First get the numbers for max depth, 32?\n",
    "3. Get the profiles in the same json format - latency_curves.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check for batchsize 1 and 2 for higher max_new_tokens\n",
    "2. Batch_size 2 with max_new_tokens as 128\n",
    "3. Clearing the GPU cache before each new inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amoaballm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
